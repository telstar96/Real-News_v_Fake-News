{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment 1 (40% of grade): Text classification for Fake News Detection\n",
    "\n",
    "This coursework will involve you implementing functions for a text classifier, which you will train to detect **fake news** in a corpus of approx. 10,000 statements, which will be split into a 80%/20% training/test split. \n",
    "\n",
    "In this template you are given the basis for that implementation, though some of the functions are missing, which you have to fill in.\n",
    "\n",
    "Follow the instructions file **NLP_Assignment_1_Instructions.pdf** for details of each question - the outline of what needs to be achieved for each question is as below.\n",
    "\n",
    "You must submit all **ipython notebooks and extra resources you need to run the code if you've added them** in the code submission, and a **2 page report (pdf)** in the report submission on QMPlus where you report your methods and findings according to the instructions file for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/samson/opt/anaconda3/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /Users/samson/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/samson/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/samson/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in /Users/samson/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support # to report on precision and recall\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"Load data from a tab-separated file and append it to raw_data.\"\"\"\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[0] == \"Id\":  # skip header\n",
    "                continue\n",
    "            (label, text) = parse_data_line(line)\n",
    "            raw_data.append((text, label))\n",
    "\n",
    "def split_and_preprocess_data(percentage):\n",
    "    \"\"\"Split the data between train_data and test_data according to the percentage\n",
    "    and performs the preprocessing.\"\"\"\n",
    "    num_samples = len(raw_data)\n",
    "    num_training_samples = int((percentage * num_samples))\n",
    "    for (text, label) in raw_data[:num_training_samples]:\n",
    "        train_data.append((to_feature_vector(pre_process(text)),label))\n",
    "    for (text, label) in raw_data[num_training_samples:]:\n",
    "        test_data.append((to_feature_vector(pre_process(text)),label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Input and Basic preprocessing (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label):\n",
    "    \"\"\"Converts the multiple classes into two,\n",
    "    making it a binary distinction between fake news and real.\"\"\"\n",
    "    #return label\n",
    "    # Converting the multiclass labels to binary label\n",
    "    labels_map = {\n",
    "        'true': 'REAL',\n",
    "        'mostly-true': 'REAL',\n",
    "        'half-true': 'REAL',\n",
    "        'false': 'FAKE',\n",
    "        'barely-true': 'FAKE',\n",
    "        'pants-fire': 'FAKE'\n",
    "    }\n",
    "    return labels_map[label]\n",
    "\n",
    "\n",
    "def parse_data_line(data_line):\n",
    "    data_line_vals = data_line\n",
    "    label = convert_label(data_line_vals[1])\n",
    "    statement = data_line_vals[2]\n",
    "    # Should return a tuple of the label as just FAKE or REAL and the statement\n",
    "    # e.g. (label, statement)\n",
    "    return (label, statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input: a string of one statement\n",
    "def pre_process(text):\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([.,;:!?'\\\"“\\(\\)])(\\w)\", r\"\\1 \\2\", text)\n",
    "    tokens = re.split(r\"\\s+\",text)\n",
    "    # normalisation - only by lower casing for now\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Basic Feature Extraction (20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_feature_dict = {} # A global dictionary of features\n",
    "\n",
    "def to_feature_vector(tokens):\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    # Defining the count_vect as = to the CountVectorizer() function.\n",
    "    # This function will transform our tokens into a matrix, with each row corresponding to the statement and the columns to the number for features for each token.\n",
    "    # It will also give us the frequency of each token as it's weight.\n",
    "    # To create the dictionary, we can use the .vocabulary_ to method to assign the keys and it's value.\n",
    "    # We then update our global dictionary using vocab variable.\n",
    "    # The function will return vocab (i.e a dictionary that has been transformed with the CountVectorizer)\n",
    "    count_vect = CountVectorizer()\n",
    "    count_vect.fit_transform(tokens)\n",
    "    vocab = count_vect.vocabulary_\n",
    "    global_feature_dict.update(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "\n",
    "def train_classifier(data):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Cross-validation (20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #solution\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# def cross_validate(dataset, folds):\n",
    "#     results = []\n",
    "#     fold_size = int(len(dataset)/folds) + 1 #length of dataset/10 + 1 to split the data evenly. so 10 + 1. We add one to allow us to have 10 folds, as python starts at 0.\n",
    "    \n",
    "#     for i in range(0,len(dataset),int(fold_size)):\n",
    "#         print(\"Fold start on items %d - %d\" % (i, i+fold_size))\n",
    "#         train = dataset[0:i] + dataset[i+fold_size:]\n",
    "#         val = dataset[i:i+fold_size]\n",
    "#         classifier = train_classifier(train)\n",
    "#         predicted_labels = predict_labels(val, classifier)\n",
    "        \n",
    "#         acc = accuracy_score(pred_values , y_test)\n",
    "        \n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def cross_validate(dataset, folds):\n",
    "    results = []\n",
    "    fold_size = int(len(dataset)/folds) + 1 #length of dataset/10 + 1 to split the data evenly. so 10 + 1. We add one to allow us to have 10 folds, as python starts at 0.\n",
    "    \n",
    "    for i in range(0,len(dataset),int(fold_size)): # range(start at 0, stop at the end of the dataset, for each )\n",
    "        # insert code here that trains and tests on the 10 folds of data in the dataset\n",
    "        \n",
    "        print(\"Fold start on items %d - %d\" % (i, i+fold_size))\n",
    "        # FILL IN THE METHOD HERE\n",
    "        train_sample = dataset[:i] + dataset[i+fold_size:]\n",
    "        test_sample = dataset[i:i+fold_size]\n",
    "        cv = train_classifier(train_sample)\n",
    "        test_sample_x = [i[0] for i in test_sample]\n",
    "        test_sample_y = [i[1] for i in test_sample]\n",
    "        predicted_labels = predict_labels(test_sample_x,cv)\n",
    "        cv_results = classification_report(test_sample_y,predicted_labels)\n",
    "        print(cv_results)\n",
    "        results.append(cv_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predict_labels(samples, classifier):\n",
    "    \"\"\"Assuming preprocessed samples, return their predicted labels from the classifier model.\"\"\"\n",
    "    return classifier.classify_many(samples)\n",
    "\n",
    "def predict_label_from_raw(sample, classifier):\n",
    "    \"\"\"Assuming raw text, return its predicted label from the classifier model.\"\"\"\n",
    "    return classifier.classify(to_feature_vector(preProcess(reviewSample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 10241 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 10241 rawData, 8192 trainData, 2049 testData\n",
      "Training Samples: \n",
      "8192\n",
      "Features: \n",
      "12193\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "raw_data = []          # the filtered data from the dataset file\n",
    "train_data = []        # the pre-processed training data as a percentage of the total dataset\n",
    "test_data = []         # the pre-processed test data as a percentage of the total dataset\n",
    "\n",
    "\n",
    "# references to the data files\n",
    "data_file_path = 'fake_news.tsv'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Training Samples: \", len(train_data), \"Features: \", len(global_feature_dict), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold start on items 0 - 820\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.45      0.47      0.46       339\n",
      "        REAL       0.62      0.60      0.61       481\n",
      "\n",
      "    accuracy                           0.55       820\n",
      "   macro avg       0.54      0.54      0.54       820\n",
      "weighted avg       0.55      0.55      0.55       820\n",
      "\n",
      "Fold start on items 820 - 1640\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.51      0.53      0.52       375\n",
      "        REAL       0.59      0.56      0.57       445\n",
      "\n",
      "    accuracy                           0.55       820\n",
      "   macro avg       0.55      0.55      0.55       820\n",
      "weighted avg       0.55      0.55      0.55       820\n",
      "\n",
      "Fold start on items 1640 - 2460\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.53      0.49      0.51       392\n",
      "        REAL       0.57      0.60      0.58       428\n",
      "\n",
      "    accuracy                           0.55       820\n",
      "   macro avg       0.55      0.55      0.55       820\n",
      "weighted avg       0.55      0.55      0.55       820\n",
      "\n",
      "Fold start on items 2460 - 3280\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.48      0.50      0.49       357\n",
      "        REAL       0.60      0.58      0.59       463\n",
      "\n",
      "    accuracy                           0.54       820\n",
      "   macro avg       0.54      0.54      0.54       820\n",
      "weighted avg       0.55      0.54      0.54       820\n",
      "\n",
      "Fold start on items 3280 - 4100\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.48      0.50      0.49       348\n",
      "        REAL       0.62      0.60      0.61       472\n",
      "\n",
      "    accuracy                           0.56       820\n",
      "   macro avg       0.55      0.55      0.55       820\n",
      "weighted avg       0.56      0.56      0.56       820\n",
      "\n",
      "Fold start on items 4100 - 4920\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.48      0.51      0.49       349\n",
      "        REAL       0.62      0.58      0.60       471\n",
      "\n",
      "    accuracy                           0.55       820\n",
      "   macro avg       0.55      0.55      0.55       820\n",
      "weighted avg       0.56      0.55      0.56       820\n",
      "\n",
      "Fold start on items 4920 - 5740\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.45      0.48      0.46       349\n",
      "        REAL       0.59      0.57      0.58       471\n",
      "\n",
      "    accuracy                           0.53       820\n",
      "   macro avg       0.52      0.52      0.52       820\n",
      "weighted avg       0.53      0.53      0.53       820\n",
      "\n",
      "Fold start on items 5740 - 6560\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.50      0.51      0.51       357\n",
      "        REAL       0.62      0.60      0.61       463\n",
      "\n",
      "    accuracy                           0.56       820\n",
      "   macro avg       0.56      0.56      0.56       820\n",
      "weighted avg       0.57      0.56      0.57       820\n",
      "\n",
      "Fold start on items 6560 - 7380\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.47      0.49      0.48       363\n",
      "        REAL       0.58      0.56      0.57       457\n",
      "\n",
      "    accuracy                           0.53       820\n",
      "   macro avg       0.53      0.53      0.53       820\n",
      "weighted avg       0.53      0.53      0.53       820\n",
      "\n",
      "Fold start on items 7380 - 8200\n",
      "Training Classifier...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.46      0.53      0.49       333\n",
      "        REAL       0.64      0.57      0.60       479\n",
      "\n",
      "    accuracy                           0.56       812\n",
      "   macro avg       0.55      0.55      0.55       812\n",
      "weighted avg       0.57      0.56      0.56       812\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['              precision    recall  f1-score   support\\n\\n        FAKE       0.45      0.47      0.46       339\\n        REAL       0.62      0.60      0.61       481\\n\\n    accuracy                           0.55       820\\n   macro avg       0.54      0.54      0.54       820\\nweighted avg       0.55      0.55      0.55       820\\n',\n",
       " '              precision    recall  f1-score   support\\n\\n        FAKE       0.51      0.53      0.52       375\\n        REAL       0.59      0.56      0.57       445\\n\\n    accuracy                           0.55       820\\n   macro avg       0.55      0.55      0.55       820\\nweighted avg       0.55      0.55      0.55       820\\n',\n",
       " '              precision    recall  f1-score   support\\n\\n        FAKE       0.53      0.49      0.51       392\\n        REAL       0.57      0.60      0.58       428\\n\\n    accuracy                           0.55       820\\n   macro avg       0.55      0.55      0.55       820\\nweighted avg       0.55      0.55      0.55       820\\n',\n",
       " '              precision    recall  f1-score   support\\n\\n        FAKE       0.48      0.50      0.49       357\\n        REAL       0.60      0.58      0.59       463\\n\\n    accuracy                           0.54       820\\n   macro avg       0.54      0.54      0.54       820\\nweighted avg       0.55      0.54      0.54       820\\n',\n",
       " '              precision    recall  f1-score   support\\n\\n        FAKE       0.48      0.50      0.49       348\\n        REAL       0.62      0.60      0.61       472\\n\\n    accuracy                           0.56       820\\n   macro avg       0.55      0.55      0.55       820\\nweighted avg       0.56      0.56      0.56       820\\n',\n",
       " '              precision    recall  f1-score   support\\n\\n        FAKE       0.48      0.51      0.49       349\\n        REAL       0.62      0.58      0.60       471\\n\\n    accuracy                           0.55       820\\n   macro avg       0.55      0.55      0.55       820\\nweighted avg       0.56      0.55      0.56       820\\n',\n",
       " '              precision    recall  f1-score   support\\n\\n        FAKE       0.45      0.48      0.46       349\\n        REAL       0.59      0.57      0.58       471\\n\\n    accuracy                           0.53       820\\n   macro avg       0.52      0.52      0.52       820\\nweighted avg       0.53      0.53      0.53       820\\n',\n",
       " '              precision    recall  f1-score   support\\n\\n        FAKE       0.50      0.51      0.51       357\\n        REAL       0.62      0.60      0.61       463\\n\\n    accuracy                           0.56       820\\n   macro avg       0.56      0.56      0.56       820\\nweighted avg       0.57      0.56      0.57       820\\n',\n",
       " '              precision    recall  f1-score   support\\n\\n        FAKE       0.47      0.49      0.48       363\\n        REAL       0.58      0.56      0.57       457\\n\\n    accuracy                           0.53       820\\n   macro avg       0.53      0.53      0.53       820\\nweighted avg       0.53      0.53      0.53       820\\n',\n",
       " '              precision    recall  f1-score   support\\n\\n        FAKE       0.46      0.53      0.49       333\\n        REAL       0.64      0.57      0.60       479\\n\\n    accuracy                           0.56       812\\n   macro avg       0.55      0.55      0.55       812\\nweighted avg       0.57      0.56      0.56       812\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(train_data, 10)  # will work and output overall performance of p, r, f-score when cv implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Error Analysis (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "# a function to make the confusion matrix readable and pretty\n",
    "def confusion_matrix_heatmap(y_test, preds, labels):\n",
    "    \"\"\"Function to plot a confusion matrix\"\"\"\n",
    "    # pass labels to the confusion matrix function to ensure right order\n",
    "    cm = metrics.confusion_matrix(y_true=y_test, y_pred=preds, labels=labels)\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion matrix of the classifier')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels( labels, rotation=45)\n",
    "    ax.set_yticklabels( labels)\n",
    "\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            text = ax.text(j, i, cm[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    \n",
    "    # fix for mpl bug that cuts off top/bottom of seaborn viz:\n",
    "    b, t = plt.ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    plt.show() # ta-da!\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        FAKE       0.49      0.49      0.49       926\n",
      "        REAL       0.58      0.57      0.57      1123\n",
      "\n",
      "    accuracy                           0.53      2049\n",
      "   macro avg       0.53      0.53      0.53      2049\n",
      "weighted avg       0.54      0.53      0.54      2049\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cv = train_classifier(train_data)\n",
    "test_x = [i[0] for i in test_data]\n",
    "test_y = [i[1] for i in test_data]\n",
    "pred = predict_labels(test_x,cv)\n",
    "cv_results = classification_report(test_y,pred)\n",
    "\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Looking at the 7th fold as it performed the worst for fake news.\n",
    "# fold_size = int((len(test_data)/10)+1)\n",
    "# test_sample = test_data[0:0+fold_size]\n",
    "# err_test_x = [i[0] for i in test_sample]\n",
    "# err_test_y = [i[1] for i in test_sample]\n",
    "# err_pred = predict_labels(err_test_x,cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the 7th fold as it performed the worst for fake news.\n",
    "fold_size = int((len(test_data)/10)+1)\n",
    "test_sample = test_data[(fold_size*6):(fold_size*7)]\n",
    "err_test_x = [i[0] for i in test_sample]\n",
    "err_test_y = [i[1] for i in test_sample]\n",
    "err_pred = predict_labels(err_test_x,cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(err_pred)):\n",
    "    # If the predicted label is Real\n",
    "    if pred[i] == 'REAL':\n",
    "        if err_test_y[i] == 'FAKE': # and the real label is Fake\n",
    "            with open('FP.txt', 'a') as f:\n",
    "                print('\\nfalse positive => predicted: REAL => ground truth: FAKE\\n' ,err_test_x[i], file=f)\n",
    "    # Else meaing that the predicted label is Fake            \n",
    "    else:\n",
    "        if err_test_y[i] == 'REAL': # and the real label is Real\n",
    "            with open('FN.txt', 'a') as f:\n",
    "                print('\\nfalse negative => predicted: FAKE => ground truth: REAL\\n',err_test_x[i], file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix_heatmap(test_y, pred, labels=['REAL','FAKE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions 5 (20%) and 6 (20%) (recommend starting a new notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'the': 9, 'bush': 0, 'tax': 8, 'cuts': 2, 'helped': 4, 'to': 10, 'create': 1, 'substantial': 7, 'part': 6, 'of': 5, 'deficit': 3}, 'REAL')\n",
      "Training Classifier...\n",
      "Done training!\n",
      "Precision: 0.536629\n",
      "Recall: 0.536847\n",
      "F Score:0.536735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samson/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the traning data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(test_data[0])   # have a look at the first test data instance\n",
    "    classifier = train_classifier(train_data)  # train the classifier\n",
    "    test_true = [t[1] for t in test_data]   # get the ground-truth labels from the data\n",
    "    test_pred = predict_labels([x[0] for x in test_data], classifier)  # classify the test data to get predicted labels\n",
    "    final_scores = precision_recall_fscore_support(test_true, test_pred, average='weighted') # evaluate\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % final_scores[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "86fb86a74c913095f2328121504a2cb38245fc0b392e5f183237bbf35130b558"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
